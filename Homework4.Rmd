---
title: "Homework4"
author: "Nico"
date: "2025-02-18"
output: pdf_document
---
UT EID: ngd455
Github: https://github.com/nicodegrandchant/Homework4_SDS315

```{r global options, include=FALSE}
# Create the global options and import the data
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

letter_frequencies <- read.csv("letter_frequencies.csv")
brown_sentences <- readLines("brown_sentences.txt")

library(tidyverse)
library(knitr)
library(ggplot2)
library(knitr)
library(mosaic)
library(kableExtra)

set.seed(42)
```

### Problem 1 - Iron Bank

```{r}
# null hypothesis: Iron Bank's trades were flagged by chance
# note, observed number of flags = 70 out of 2021
# note, legal trades are flagged at a rate of 2.4%
trades <- 2021
probFlag <- 0.024
observedFlag <- 70

#Simulation
flagged <- rbinom(100000, size = trades, prob = probFlag)
pIronBank <- mean(flagged >= observedFlag)
flaggedPlot <- data.frame(flagged)

#Plot 
ggplot(flaggedPlot, aes(x = flagged)) +
  geom_histogram(fill = "lightgreen", color = "black", binwidth = 1) + 
  geom_vline(xintercept = observedFlag, linetype = "dashed", color = "black", size = 0.5) +  # Lower bound
  labs(title = "Distribution Plot of Flagged Trades",
       subtitle = "Assuming the null hypothesis is true",
       x = "Flagged Trades", 
       y = "Count")
```

The null hypothesis being tested is that Iron Bank's trades were flagged by chance. In order to measure evidence against this null hypothesis, we measured the number of trades flagged out of 2021 transactions. This follows a binomial model where the probability of a transaction being flagged is about 2.4%. To see if their flagged trades were in fact caused by chance, we simulated 100,000 times the same number of transactions with the same rate of flagged transactions to see how many simulations have 70 or more trades get flagged. Finding the proportion of this, we get our p value of 0.0019. Graphically, you can see almost no simulation has more than or equal to 70 transactions flagged. 
Considering it is reasonably below the common threshold of 0.05, the null hypothesis does not look plausible as the chance is extremely low and proves that 70 flagged trades is statistically significant.

### Problem 2
Question: Are the observed data for Gourmet Bites consistent with the Health Department’s null hypothesis that, on average, restaurants in the city are cited for health code violations at the same 3% baseline rate?

Follow the same answer format as the previous question

```{r}
# null hypothesis: on average, restaurants in the city are cited for health code violations at the same 3% baseline rate
# note, observed number of violations is 8 

inspections <- 50
probViolation <- 0.03
observedViolation <- 8

#Simulation
violation <- rbinom(100000, size = inspections, prob = probViolation)
pIronBank <- mean(violation >= observedViolation)
violationPlot <- data.frame(violation)
```

The null hypothesis being tested is that on average, restaurants in the city are cited for health code violations at the same 3% baseline rate. In order to measure evidence against this null hypothesis, we measured the number of times Gourmet Bites was flagged out of 50 inspections This follows a binomial model where the probability of a restaurant having a violation is about 3%. To see if Gourmet Bites violations were in fact caused by chance, we simulated 100,000 times the same number of inspections with the same rate of violations to see how many simulations have 8 or more inspections get violations. Finding the proportion of this, we get our p value of 0.00011. 
Considering it is reasonably below the common threshold of 0.05, the null hypothesis does not look plausible as the chance is extremely low and proves that 8 inspections with violations is statistically significant.

### Problem 3

```{r}
# Null Hypothesis H0: The distribution of empaneled jurors is similar to the country's proportions.
# Alternative Hypothesis HA: The distribution of empaneled jurors is different to the country's population proportions.

empaneledByCat <- c(85, 56, 59, 27, 13)
proportions <- c(0.30, 0.25, 0.20, 0.15, 0.10)

# Chi Squared Goodness of fit test, Oi being the observed count in each category, and Ei being the expected count in each category
totalJuries <- sum(empaneledByCat)
expectedJuries <- totalJuries * proportions

chisqJuries <- sum((empaneledByCat - expectedJuries)^2 / expectedJuries)

# Probability of obtaining various values of T when H0 holds P(T | H0)
pValueJuries <- pchisq(chisqJuries, df = 4, lower.tail = FALSE)
```

Null Hypothesis H0: The distribution of empaneled jurors is similar to the country's population proportions.

Alternative Hypothesis HA: The distribution of empaneled jurors is different to the country's population proportions.

Chi Squared Goodness of fit test, Oi being the observed count in each category, and Ei being the expected count in each category.
`r chisqJuries`

Probability of obtaining various values of T when the Null Hypothesis holds true. 
`r pValueJuries`

With a p value of around 0.014 being less 0.05, our alternative hypothesis seems more accurate than our null hypothesis, and this information is statistically significant. Furthermore, this means potentially systematic bias in jury selection could be present and that the distribution of empaneled jurors is different to the country's proportions. This could be due to other factors like certain ethnic groups possessing a higher rate of "for cause" removals due to biases in comparison to others, although I would suggest to investigate this variable amongst others to be sure of exactly why this discrepancy exists, and if systematic discrimination is present in the jury selection process.

### Problem 4

Part A:
```{r}

# Function to clean text and count letter occurrences
letter_cleaning <- function(sentence, letterTable) {
  letter_levels <- toupper(trimws(letterTable$Letter))  # Get letter list from table
  clean_sentence <- toupper(gsub("[^A-Za-z]", "", sentence))
  letters_vec <- strsplit(clean_sentence, "")[[1]]
  observed_counts <- table(factor(letters_vec, levels = letter_levels))

  return(as.numeric(observed_counts))
}

# Create function to find chi squared
chi_squared <- function(sentence, letterTable) {
  observed <- letter_cleaning(sentence, letterTable)
  sentence_length <- sum(observed)
  expected <- sentence_length * letterTable$Probability
  chi_sq <- sum((observed - expected)^2 / expected)
  
  return(chi_sq)
}

# Compute chi-squared values for all sentences
null_distribution <- sapply(brown_sentences, function(sentence) {
  chi_squared(sentence, letter_frequencies)
})

# Convert to a data frame
null_distribution_df <- data.frame(Chi_Squared = null_distribution)

ggplot(null_distribution_df, aes(x = Chi_Squared)) +
  geom_histogram(binwidth = 2, fill = "lightblue", color = "black") +
  labs(title = "Chi-Squared Null Distribution",
       x = "Chi-Squared Statistic",
       y = "Frequency") +
  theme_minimal()
```

Part B:

```{r}
test_sentences <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum’s new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker’s inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project's effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone's expectations."
)

# Compute chi-squared and p values for test sentences
test_chi_squared <- function(sentence, letterTable) {
  observed <- letter_cleaning(sentence, letterTable)  
  sentence_length <- sum(observed)
  expected <- sentence_length * letterTable$Probability
  chi_sq_sentences <- sum((observed - expected)^2 / expected)
  return(chi_sq_sentences)
}

test_chi_squared_sentences <- sapply(test_sentences, function(sentence) {
  test_chi_squared(sentence, letter_frequencies)
})

test_p_values <- sapply(test_chi_squared_sentences, function(x) {
  mean(null_distribution >= x)  
})

# store the results and print them, 
sentence_result <- data.frame(
  Sentence_ID = 1:10,
  Chi_Squared = round(test_chi_squared_sentences, 3),  
  P_Value = round(test_p_values, 3) 
)

colnames(sentence_result) <- c("ID", "Chi^2", "P-Value")
kable(sentence_result, format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "hold_position") %>%
  column_spec(1, width = "6cm") %>%   # Adjust column width for "ID"
  column_spec(2, width = "4cm") %>%   # Adjust width for "Chi²"
  column_spec(3, width = "4cm") 

```

The sentence produced by an LLM and watermarked by asking the LLM to subtly adjust the frequency distribution over letters is sentence number 6, or the following:

Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland

We know this because of the 0.009 p value below 0.05, meaning that there is only a 0.9% chance that a sentence with typical English letter distributions would have a chi-squared statistic as different as this one.
